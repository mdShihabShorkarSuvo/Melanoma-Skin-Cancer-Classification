# -*- coding: utf-8 -*-
"""hamrairremoval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kLZU35Bt8pk2RsHMv1olMuxGpjJTJf3D
"""

!pip install tensorflow
!pip install --upgrade tensorflow keras

!nvidia-smi

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.applications import  ResNet50, DenseNet121, InceptionV3, EfficientNetB0, MobileNetV2, Xception, NASNetMobile
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

# Check GPU availability and memory
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("GPU found and enabled for training!")
    except RuntimeError as e:
        print(e)

# Paths
dataset_dir = "/content/drive/MyDrive/dataset 3 and 6/DataSet Skin Cancer/ham10000-hair remove"
output_dir = "/content/drive/MyDrive/trained_models"

# Dictionary of models
models_dict = {

    'ResNet50': ResNet50,
    'DenseNet121': DenseNet121,
    'InceptionV3': InceptionV3,
    'EfficientNetB0': EfficientNetB0,
    'MobileNetV2': MobileNetV2,
    'Xception': Xception,
    'NASNetMobile': NASNetMobile
}

# Parameters
input_size = (150, 150)  # Smaller input size for faster training
batch_size = 128  # Optimized batch size (adjustable)
epochs = 5

# Data generator with balanced folder representation
datagen = ImageDataGenerator(
    rescale=1.0 / 255,  # Normalize pixel values
    rotation_range=20,  # Augment data
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Train generator
train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True  # Ensures balanced batches across classes
)

# Function to build and compile a model
# Function to build and compile a model
def build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7): # Changed num_classes to 7
    base_model = base_model_function(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the base model
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='softmax')  # Output layer now matches the number of classes
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Train each model sequentially
for model_name, base_model_function in models_dict.items():
    print(f"Training {model_name}...")

    # Build the model
    model = build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7) # Changed num_classes to 7

    # Train the model
    history = model.fit(
        train_generator,
        epochs=epochs,
        verbose=1
    )

    # Save the model
    save_path = os.path.join(output_dir, f"{model_name}_trained.h5")
    model.save(save_path)
    print(f"{model_name} model saved to {save_path}\n")

print("All models have been trained!")

from google.colab import drive

# Use force_remount=True to override the existing mount
drive.mount('/content/drive', force_remount=True)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.applications import (
    VGG16,
    ResNet50,

)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.metrics import Precision, Recall
import os
import matplotlib.pyplot as plt

# Check GPU availability and memory
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("GPU found and enabled for training!")
    except RuntimeError as e:
        print(e)

# Paths
dataset_dir = "/content/drive/MyDrive/dataset 3 and 6/DataSet Skin Cancer/ham10000-hair remove"
output_dir = "/content/drive/MyDrive/trained_models"

# Dictionary of models
# Dictionary of models
models_dict = {
    'VGG16': VGG16,
    'ResNet50': ResNet50
}

# Parameters
input_size = (150, 150)  # Adjust input size to match model requirements
batch_size = 256  # Optimized batch size (adjustable)
epochs = 5

# Data generator with balanced folder representation
datagen = ImageDataGenerator(
    rescale=1.0 / 255,  # Normalize pixel values
    rotation_range=20,  # Augment data
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Train generator
train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True  # Ensures balanced batches across classes
)

# Function to build and compile a model
def build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7):
    base_model = base_model_function(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the base model
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='softmax')  # Output layer now matches the number of classes
    ])
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
    )
    return model

# Train each model sequentially and visualize metrics
for model_name, base_model_function in models_dict.items():
    print(f"Training {model_name}...")

    # Build the model
    model = build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7)

    # Train the model
    history = model.fit(
        train_generator,
        epochs=epochs,
        verbose=1
    )

    # Calculate metrics
    accuracy = history.history['accuracy'][-1]
    precision = history.history['precision'][-1]
    recall = history.history['recall'][-1]
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)  # Add epsilon to avoid division by zero

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}")

    # Save the model
    save_path = os.path.join(output_dir, f"{model_name}_trained.h5")
    model.save(save_path)
    print(f"{model_name} model saved to {save_path}\n")

    # Plotting the results as a bar chart
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1_score]
    colors = ['blue', 'green', 'orange', 'red']

    plt.figure(figsize=(8, 6))
    plt.bar(metrics, values, color=colors)
    plt.title(f'Metrics for {model_name}')
    plt.ylim(0, 1)  # Metrics range from 0 to 1
    plt.ylabel('Value')
    plt.xlabel('Metrics')
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontsize=12)
    plt.show()

print("All models have been trained and visualized!")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.applications import (
    VGG16,
    ResNet50,
    DenseNet121,
    InceptionV3,
    EfficientNetB0,
    MobileNetV2,
    Xception,
    NASNetMobile
)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.metrics import Precision, Recall
import os
import matplotlib.pyplot as plt

# Check GPU availability and memory
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("GPU found and enabled for training!")
    except RuntimeError as e:
        print(e)

# Paths
dataset_dir = "/content/drive/MyDrive/dataset 3 and 6/DataSet Skin Cancer/ham10000-hair remove"
output_dir = "/content/drive/MyDrive/trained_models"

# Dictionary of models
models_dict = {

    'DenseNet121': DenseNet121,
    'InceptionV3': InceptionV3,
    'EfficientNetB0': EfficientNetB0,
    'MobileNetV2': MobileNetV2,
    'Xception': Xception,
    'NASNetMobile': NASNetMobile,
}

# Parameters
input_size = (150, 150)  # Adjust input size to match model requirements
batch_size = 256  # Optimized batch size (adjustable)
epochs = 5

# Data generator with balanced folder representation
datagen = ImageDataGenerator(
    rescale=1.0 / 255,  # Normalize pixel values
    rotation_range=20,  # Augment data
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Train generator
train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True  # Ensures balanced batches across classes
)

# Function to build and compile a model
def build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7):
    base_model = base_model_function(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the base model
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='softmax')  # Output layer now matches the number of classes
    ])
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
    )
    return model

# Train each model sequentially and visualize metrics
for model_name, base_model_function in models_dict.items():
    print(f"Training {model_name}...")

    # Build the model
    model = build_model(base_model_function, input_shape=(150, 150, 3), num_classes=7)

    # Train the model
    history = model.fit(
        train_generator,
        epochs=epochs,
        verbose=1
    )

    # Calculate metrics
    accuracy = history.history['accuracy'][-1]
    precision = history.history['precision'][-1]
    recall = history.history['recall'][-1]
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)  # Add epsilon to avoid division by zero

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}")

    # Save the model
    save_path = os.path.join(output_dir, f"{model_name}_trained.h5")
    model.save(save_path)
    print(f"{model_name} model saved to {save_path}\n")

    # Plotting the results as a bar chart
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1_score]
    colors = ['blue', 'green', 'orange', 'red']

    plt.figure(figsize=(8, 6))
    plt.bar(metrics, values, color=colors)
    plt.title(f'Metrics for {model_name}')
    plt.ylim(0, 1)  # Metrics range from 0 to 1
    plt.ylabel('Value')
    plt.xlabel('Metrics')
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontsize=12)
    plt.show()

print("All models have been trained and visualized!")

from google.colab import drive

# Use force_remount=True to override the existing mount
drive.mount('/content/drive', force_remount=True)

# prompt: for the last code i want all the  model result indivuidal like i run the vgg then i will get the accuracy , precsiom recall and f1 result then plot it then it will follow the same for every other model

from google.colab import drive
import os
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import matplotlib.image as mpimg
from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import (VGG16, ResNet50, DenseNet121, InceptionV3, EfficientNetB0, MobileNetV2, Xception, NASNetMobile)
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.callbacks import EarlyStopping




# ... (Your existing code)

# Check GPU availability
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("GPU found and enabled for training!")
    except RuntimeError as e:
        print(e)

# Paths
dataset_dir = "/content/drive/MyDrive/dataset 3 and 6/DataSet Skin Cancer/ham10000-hair remove"
output_dir = "/content/drive/MyDrive/trained_models"

# Parameters
input_size = (128, 128)  # Increased input size for better feature representation
batch_size = 32
num_classes =7  # Assuming 6 classes in your dataset
epochs = 1 # Increased epochs for better training
patience = 5  # Early stopping patience

# Data generator with augmentation
datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=40,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode='nearest',
    validation_split=0.2  # 20% data for validation
)

train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True,
    subset='training'  # Training data subset
)

validation_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True,
    subset='validation'  # Validation data subset
)

# Function to build and compile models
def build_model(base_model_function, input_shape=(128, 128, 3), num_classes=7):
    base_model = base_model_function(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the base model
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
    )
    return model

# List of models
models_dict = {
    'DenseNet121': tf.keras.applications.DenseNet121
}

# Store metrics for all models
all_metrics = {}

# Training and evaluation
for model_name, base_model_function in models_dict.items():
    print(f"Training {model_name}...")

    # Build the model
    model = build_model(base_model_function, input_shape=input_size + (3,), num_classes=num_classes)

    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

    # Train the model
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=epochs,
        callbacks=[early_stopping],
        verbose=1
    )

    # Calculate metrics
    best_epoch = len(history.history['accuracy']) - patience if len(history.history['accuracy']) > patience else len(history.history['accuracy'])
    accuracy = history.history['accuracy'][best_epoch - 1]
    precision = history.history['precision'][best_epoch - 1]
    recall = history.history['recall'][best_epoch - 1]
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)  # Add epsilon to avoid division by zero

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}")

    # Save metrics for plotting
    all_metrics[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1_score
    }

    # Save the model
    save_path = os.path.join(output_dir, f"{model_name}_trained.h5")
    model.save(save_path)
    print(f"{model_name} model saved to {save_path}\n")


    # Plotting the results as a bar chart
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1_score]
    colors = ['blue', 'green', 'orange', 'red']

    plt.figure(figsize=(8, 6))
    plt.bar(metrics, values, color=colors)
    plt.title(f'Metrics for {model_name}')
    plt.ylim(0, 1)  # Metrics range from 0 to 1
    plt.ylabel('Value')
    plt.xlabel('Metrics')
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontsize=12)
    plt.show()

from google.colab import drive
drive.mount('/content/drive')



# prompt: for the last code i want all the  model result indivuidal like i run the vgg then i will get the accuracy , precsiom recall and f1 result then plot it then it will follow the same for every other model

from google.colab import drive
import os
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import matplotlib.image as mpimg
from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import (VGG16, ResNet50, DenseNet121, InceptionV3, EfficientNetB0, MobileNetV2, Xception, NASNetMobile)
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.callbacks import EarlyStopping




# Paths
dataset_dir = "/content/drive/MyDrive/Light Field Image Dataset of Skin Lesions"
output_dir = "/content/drive/MyDrive/trained_models"

# Parameters
input_size = (128, 128)  # Increased input size for better feature representation
batch_size = 32
num_classes =7  # Assuming 6 classes in your dataset
epochs = 30  # Increased epochs for better training
patience = 5  # Early stopping patience

# Data generator with augmentation
datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=40,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode='nearest',
    validation_split=0.2  # 20% data for validation
)

train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True,
    subset='training'  # Training data subset
)

validation_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=input_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True,
    subset='validation'  # Validation data subset
)

# Function to build and compile models
def build_model(base_model_function, input_shape=(128, 128, 3), num_classes=7):
    base_model = base_model_function(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze the base model
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]
    )
    return model

# List of models
models_dict = {
    'VGG16': tf.keras.applications.VGG16,
    'ResNet50': tf.keras.applications.ResNet50,
    'DenseNet121': tf.keras.applications.DenseNet121,
    'InceptionV3': tf.keras.applications.InceptionV3,
    'EfficientNetB0': tf.keras.applications.EfficientNetB0,
    'MobileNetV2': tf.keras.applications.MobileNetV2,
    'Xception': tf.keras.applications.Xception,
    'NASNetMobile': tf.keras.applications.NASNetMobile,
}

# Store metrics for all models
all_metrics = {}

# Training and evaluation
for model_name, base_model_function in models_dict.items():
    print(f"Training {model_name}...")

    # Build the model
    model = build_model(base_model_function, input_shape=input_size + (3,), num_classes=num_classes)

    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

    # Train the model
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=epochs,
        callbacks=[early_stopping],
        verbose=1
    )

    # Calculate metrics
    best_epoch = len(history.history['accuracy']) - patience if len(history.history['accuracy']) > patience else len(history.history['accuracy'])
    accuracy = history.history['accuracy'][best_epoch - 1]
    precision = history.history['precision'][best_epoch - 1]
    recall = history.history['recall'][best_epoch - 1]
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)  # Add epsilon to avoid division by zero

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}")

    # Save metrics for plotting
    all_metrics[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1_score
    }

    # Save the model
    save_path = os.path.join(output_dir, f"{model_name}_trained.h5")
    model.save(save_path)
    print(f"{model_name} model saved to {save_path}\n")


    # Plotting the results as a bar chart
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [accuracy, precision, recall, f1_score]
    colors = ['blue', 'green', 'orange', 'red']

    plt.figure(figsize=(8, 6))
    plt.bar(metrics, values, color=colors)
    plt.title(f'Metrics for {model_name}')
    plt.ylim(0, 1)  # Metrics range from 0 to 1
    plt.ylabel('Value')
    plt.xlabel('Metrics')
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontsize=12)
    plt.show()